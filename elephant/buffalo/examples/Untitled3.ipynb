{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prov.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prov.graph import prov_to_graph, graph_to_prov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca.utils.files import _get_prov_file_format\n",
    "from alpaca.graph import ProvenanceGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/home/koehler/PycharmProjects/alpaca/examples/run_basic.ttl\"\n",
    "file_path = \"/home/koehler/outputs/provenance/psd_by_trial_type/R2G_PSD_all_subjects.ttl\"\n",
    "# file_path = \"/home/koehler/Desktop/var_args.ttl\"\n",
    "# file_path = \"/home/koehler/Desktop/intermediate.ttl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = ProvDocument.deserialize(file_path, format=\"rdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_doc = ProvDocument()\n",
    "for record in doc.get_records((ProvEntity, ProvActivity, ProvGeneration, ProvUsage, ProvMembership)):\n",
    "    filtered_doc.add_record(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = prov_to_graph(filtered_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prov = g.reverse(copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NID_ALPACA = \"alpaca\"\n",
    "\n",
    "NSS_FUNCTION = \"function\"             # Functions executed\n",
    "NSS_FILE = \"file\"                     # Files accessed\n",
    "NSS_DATA = \"object\"                   # Data objects (input/outputs/containers)\n",
    "NSS_SCRIPT = \"script\"                 # The execution script\n",
    "\n",
    "NSS_PARAMETER = \"parameter\"           # Function parameter\n",
    "NSS_ATTRIBUTE = \"attribute\"           # Data object attribute (i.e., class)\n",
    "NSS_ANNOTATION = \"annotation\"         # Data object annotation (e.g., Neo)\n",
    "NSS_CONTAINER = \"container\"           # For storing container membership access information\n",
    "\n",
    "\n",
    "ALL_NSS = [NSS_FUNCTION, NSS_FILE, NSS_DATA, NSS_SCRIPT, NSS_PARAMETER,\n",
    "           NSS_ATTRIBUTE, NSS_ANNOTATION, NSS_CONTAINER]\n",
    "\n",
    "NAMESPACES = [Namespace(namespace, f\"urn:{NID_ALPACA}:{namespace}:\")\n",
    "              for namespace in ALL_NSS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_namespace(uri):\n",
    "    for ns in NAMESPACES:\n",
    "        parts = uri.split(ns.uri)\n",
    "        if len(parts) > 1:\n",
    "            return ns.prefix, parts[1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_id(uri):\n",
    "    node_id = str(uri)\n",
    "    namespace = _find_namespace(node_id)\n",
    "    if namespace:\n",
    "        return \":\".join(namespace)\n",
    "    return node_id\n",
    "\n",
    "\n",
    "def _add_attribute(data, attr_name, attr_type, attr_value, strip_namespace):\n",
    "    if not strip_namespace:\n",
    "        attr_name = f\"{attr_type}:{attr_name}\"\n",
    "\n",
    "    if attr_name in data:\n",
    "        raise ValueError(\"Duplicate property values. Make sure to include the namespaces!\")\n",
    "    data[attr_name] = attr_value\n",
    "\n",
    "\n",
    "def _get_data(node, node_id, annotations=None, attributes=None, array_annotations=None, strip_namespace=True):\n",
    "    filter_map = defaultdict(list)\n",
    "\n",
    "    # Array annotations is a special attribute that stores a dictionary\n",
    "    # We will retrieve it as an ordinary attribute value, and then\n",
    "    # process later\n",
    "    if array_annotations:\n",
    "        attributes = attributes + tuple(array_annotations.keys())\n",
    "    \n",
    "    filter_map.update({NSS_ANNOTATION: annotations, NSS_ATTRIBUTE: attributes})\n",
    "    \n",
    "    data = {\"gephi_interval\": []}\n",
    "    namespace, local_part = node_id.split(\":\", 1)\n",
    "    data['type'] = \"activity\" if namespace == NSS_FUNCTION else namespace\n",
    "\n",
    "    info = local_part.split(\":\")\n",
    "    data['data_hash'] = info[-1]\n",
    "    if namespace == NSS_FILE:\n",
    "        data['label'] = \"File\"\n",
    "        data['hash_type'] = info[-2]\n",
    "    elif namespace == NSS_DATA:\n",
    "        data['label'] = info[-2].split(\".\")[-1]\n",
    "        data['Python_name'] = info[-2]\n",
    "    \n",
    "    for attr in node.attributes:\n",
    "        attr_type = str(attr[0].namespace.prefix)\n",
    "        attr_name = attr[0].localpart\n",
    "        attr_value = attr[1]\n",
    "           \n",
    "        if annotations or attributes or array_annotations:\n",
    "             if attr_name in filter_map[attr_type]:\n",
    "                if attr_name in array_annotations:\n",
    "                    # Extract the relevant keys from the array annotations string\n",
    "                    for annotation in array_annotations[attr_name]:\n",
    "                        search_annotation = re.compile(fr\"'({annotation})':\\s([\\w\\s()\\/\\-.*\\[\\]'\\,=<>]+)(,\\s'.+':|,*\\}})\")\n",
    "\n",
    "                        match = search_annotation.search(attr_value)\n",
    "\n",
    "                        if match:\n",
    "                            if match.group(1) == annotation:\n",
    "                                value = str(match.group(2))\n",
    "                                value = re.sub(r\"\\s+\", \" \", value)\n",
    "                            else:\n",
    "                                value = \"<could not fetch annotation value>\"\n",
    "\n",
    "                            _add_attribute(data, annotation, attr_type, value, strip_namespace)\n",
    "                    \n",
    "                else:\n",
    "                    _add_attribute(data, attr_name, attr_type, attr_value, strip_namespace)\n",
    "                \n",
    "                \n",
    "        if namespace == NSS_FILE and attr_type in (\"rdfs\", \"prov\") and attr_name == \"label\":\n",
    "            data[\"File_path\"] = attr_value\n",
    "            \n",
    "    return data\n",
    "    \n",
    "    \n",
    "def _add_gephi_interval(data, order):\n",
    "    if not \"gephi_interval\" in data:\n",
    "        data[\"gephi_interval\"] = []\n",
    "    data[\"gephi_interval\"].append((order, order))\n",
    "    \n",
    "\n",
    "def _get_function_call_node(u_id, v_id, relation, use_name_in_parameter=True):\n",
    "    node_id = v_id if isinstance(relation, ProvUsage) else u_id\n",
    "    \n",
    "    data = {}\n",
    "    data[\"Python_name\"] = node_id.split(\":\")[-1]\n",
    "    data[\"label\"] = data[\"Python_name\"].split(\".\")[-1]\n",
    "    data[\"type\"] = NSS_FUNCTION\n",
    "\n",
    "    for attr in relation.attributes:\n",
    "        attr_type = str(attr[0].namespace.prefix)\n",
    "        attr_name = attr[0].localpart\n",
    "        attr_value = attr[1]\n",
    "\n",
    "        if attr_type == \"prov\" and attr_name == \"value\":\n",
    "            data[\"execution_order\"] = attr_value\n",
    "        elif attr_type == NSS_PARAMETER:\n",
    "            prefix = attr_type if not use_name_in_parameter else data[\"label\"]\n",
    "            data[f\"{prefix}:{attr_name}\"] = attr_value\n",
    "\n",
    "    _add_gephi_interval(data, data[\"execution_order\"])\n",
    "    node_hash = hash((node_id, data[\"execution_order\"]))\n",
    "\n",
    "    return node_hash, data\n",
    "    \n",
    "\n",
    "def _get_membership_relation(relation, graph):\n",
    "    # Retrieve the relevant parameter (index/slice, attribute)\n",
    "    # The entity that is inside the collection has an attribute\n",
    "    # with the namespace `container:`, with the index/slice or\n",
    "    # attribute name information\n",
    "\n",
    "    member = None\n",
    "    for attr in relation.attributes:\n",
    "        if str(attr[0]) == \"prov:entity\":\n",
    "            member = attr[1].uri\n",
    "    \n",
    "    for node in graph.nodes:\n",
    "        if node.identifier.uri == member:\n",
    "            # ProvEntity object of the collection's member\n",
    "            for attr in node.attributes:\n",
    "                container_attribute = str(attr[0])\n",
    "                if container_attribute.startswith(f\"{NSS_CONTAINER}:\"):\n",
    "                    # This is the membership ProvEntity attribute\n",
    "                    # Get the attribute name or index/slice value\n",
    "                    member_type = container_attribute.split(\":\")[-1]\n",
    "                    if member_type == \"attribute\":\n",
    "                        return f\".{attr[1]}\"\n",
    "                    else:\n",
    "                        return f\"[{attr[1]}]\"\n",
    "    \n",
    "def _transform_graph(graph, annotations=None, attributes=None, array_annotations=None, strip_namespace=True, remove_none=True):\n",
    "    # Transform a NetworkX graph obtained from the PROV data, so that the visualization\n",
    "    # is simplified. A new `nx.DiGraph` object is created and returned\n",
    "    # Annotations and attributes of the entities stored in the PROV file can be filtered\n",
    "    \n",
    "    transformed = nx.DiGraph()\n",
    "    none_nodes = []\n",
    "    \n",
    "    print(\"Transforming nodes\")\n",
    "    # Copy all the nodes, changing the URI to string and extracting the requested attributes/annotations as node data\n",
    "    for node in graph.nodes:\n",
    "        node_id = _get_id(node.identifier.uri)\n",
    "        if remove_none and \"builtins.NoneType\" in node_id:\n",
    "            none_nodes.append(node)\n",
    "            continue\n",
    "        data = _get_data(node, node_id,\n",
    "                         annotations=annotations,\n",
    "                         attributes=attributes,\n",
    "                         array_annotations=array_annotations,\n",
    "                         strip_namespace=strip_namespace)\n",
    "        transformed.add_node(node_id, **data)\n",
    "    \n",
    "    print(\"Transforming edges\")\n",
    "    # Add all the edges\n",
    "    # If membership, the direction must be reversed\n",
    "    # If usage/generation, create additional nodes for the function call, with the parameters as node data\n",
    "    # A membership flag is created, as this will be used\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        if remove_none and len(none_nodes) > 0:\n",
    "            if u in none_nodes or v in none_nodes:\n",
    "                continue\n",
    "        \n",
    "        u_id = _get_id(u.identifier.uri)\n",
    "        v_id = _get_id(v.identifier.uri)\n",
    "        \n",
    "        relation = data['relation']\n",
    "        if isinstance(relation, ProvMembership):\n",
    "            membership_relation = _get_membership_relation(relation, graph)\n",
    "            transformed.add_edge(v_id, u_id, membership=True, label=membership_relation)\n",
    "        elif isinstance(relation, (ProvUsage, ProvGeneration)):    \n",
    "            node_id, node_data = _get_function_call_node(u_id, v_id, relation)\n",
    "            if not node_id in graph.nodes:\n",
    "                transformed.add_node(node_id, **node_data)\n",
    "\n",
    "            if isinstance(relation, ProvUsage):\n",
    "                transformed.add_edge(u_id, node_id, membership=False)\n",
    "                _add_gephi_interval(transformed.nodes[u_id], node_data['execution_order'])\n",
    "            else:\n",
    "                transformed.add_edge(node_id, v_id, membership=False)\n",
    "                _add_gephi_interval(transformed.nodes[v_id], node_data['execution_order'])\n",
    "        \n",
    "    print(\"Removing activities\")\n",
    "    # Remove old ProvActivity nodes that are not needed anymore (unconnected)\n",
    "    # They were set with the `type` as `activity` in the node data dictionary\n",
    "    filter_nodes = [node for node, data in transformed.nodes(data=True) if data['type'] == \"activity\"]\n",
    "    transformed.remove_nodes_from(filter_nodes)\n",
    "   \n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_missing_intervals(graph, node=None):\n",
    "    # Find all membership nodes and create a subgraph with the ancestors and successors\n",
    "    # We will have all the spots where the execution counter was not set\n",
    "    # Then we build, for each path from the bottom to the top, the full list with intervals at each node\n",
    "    processed_nodes = []\n",
    "    subgraph_nodes = []\n",
    "    for u, v, data in graph.edges(data=True):\n",
    "        if data['membership']:\n",
    "            subgraph_nodes.extend([u, v])\n",
    "            \n",
    "    subgraph = graph.subgraph(subgraph_nodes).reverse(copy=True)\n",
    "    \n",
    "    # We do progressively, changing the successors of the root nodes\n",
    "    # each time, and generating a new subgraph until no nodes remain\n",
    "    while not nx.is_empty(subgraph):\n",
    "        root_nodes = [node for node in subgraph.nodes if subgraph.in_degree(node) == 0]\n",
    "        for root in root_nodes:\n",
    "            successors = subgraph.successors(root)\n",
    "            interval = subgraph.nodes[root][\"gephi_interval\"]\n",
    "            for succ in successors:\n",
    "                graph.nodes[succ][\"gephi_interval\"].extend(interval)\n",
    "\n",
    "        processed_nodes.extend(root_nodes)\n",
    "        subgraph_nodes = []\n",
    "        for u, v, data in graph.edges(data=True):\n",
    "            if data['membership'] and v not in processed_nodes:\n",
    "                subgraph_nodes.extend([u, v])\n",
    "            \n",
    "        subgraph = graph.subgraph(subgraph_nodes).reverse(copy=True)\n",
    "            \n",
    "\n",
    "def _generate_interval_strings(graph):\n",
    "    for node, data in graph.nodes(data=True):\n",
    "        data[\"gephi_interval\"].sort(key=lambda tup: tup[0])\n",
    "        segments = \";\".join([f\"[{start:.1f},{stop:.1f}]\" for start, stop in data[\"gephi_interval\"]])\n",
    "        interval = f\"<{segments}>\"\n",
    "        data[\"Time Interval\"] = interval\n",
    "        data.pop(\"gephi_interval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming nodes\n",
      "Transforming edges\n",
      "Removing activities\n"
     ]
    }
   ],
   "source": [
    "prov_graph = _transform_graph(raw_prov, attributes=('dtype', 'shape', 'name', 'units', 'name', 'description', 'file_origin', 't_start', 't_stop'), annotations=('subject_name',),\n",
    "                              array_annotations={'array_annotations': ('channel_names',)}, strip_namespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_find_missing_intervals(prov_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_generate_interval_strings(prov_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(prov_graph, \"/home/koehler/Desktop/teste.graphml\")\n",
    "nx.write_gexf(prov_graph, \"/home/koehler/Desktop/teste.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _condense_memberships(graph, preserve=None):\n",
    "    if preserve is None:\n",
    "        preserve = []\n",
    "        \n",
    "    # Find all membership edges\n",
    "    filter_edges = [tuple(e) for *e, data in graph.edges(data=True) if data['membership']]\n",
    "    \n",
    "    # Iterate over the edges. We will contract if:\n",
    "    #  - target does not have an edge to a function\n",
    "    #  - target is not preserved\n",
    "    \n",
    "    remove_nodes = []\n",
    "    replaced_edges = []\n",
    "\n",
    "    while len(filter_edges) > 0:\n",
    "\n",
    "        e = filter_edges.pop(0)\n",
    "        if e in replaced_edges:\n",
    "            continue\n",
    "        u, v = e\n",
    "\n",
    "        if graph.nodes[v]['label'] in preserve:\n",
    "            continue\n",
    "\n",
    "        successors = []\n",
    "        input_to_function = False\n",
    "        for successor in graph.successors(v):\n",
    "            if graph.nodes[successor]['type'] == NSS_FUNCTION:\n",
    "                input_to_function = True\n",
    "                break\n",
    "            successors.append(successor)\n",
    "        if input_to_function:\n",
    "            continue\n",
    "\n",
    "        edge_data = graph.edges[e]\n",
    "  \n",
    "        # For each successor of node `v`, we connect with node `u`.\n",
    "        # We push the replaced edges to processed edges, in case they are also to be removed later.\n",
    "        # Edge label is formed by concatenating the current edge with the current value of `v` to\n",
    "        # the successor.\n",
    "        # We add the new edge to the list to be processed, in case several sequential memberships\n",
    "        # are being pruned.\n",
    "        # Replaced edges are removed from the graph.\n",
    "        for successor in successors:\n",
    "            # Create new label\n",
    "            replaced_edge = (v, successor)\n",
    "            replaced_data = graph.edges[replaced_edge]\n",
    "            new_edge_label = edge_data['label'] + replaced_data['label']\n",
    "            replaced_data['label'] = new_edge_label\n",
    "\n",
    "            # Create new edge\n",
    "            new_edge = (u, successor)\n",
    "            graph.add_edge(*new_edge, **replaced_data)\n",
    "            filter_edges.append(new_edge)\n",
    "            \n",
    "            # Remove replaced edges\n",
    "            graph.remove_edge(*replaced_edge)\n",
    "            replaced_edges.append(replaced_edge)\n",
    "            \n",
    "        \n",
    "        # Remove original edge\n",
    "        graph.remove_edge(*e)\n",
    "        \n",
    "        if not v in remove_nodes:\n",
    "            remove_nodes.append(v)\n",
    "        \n",
    "    # Remove the nodes\n",
    "    for node in remove_nodes:\n",
    "        graph.remove_node(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned = prov_graph.copy()\n",
    "_condense_memberships(pruned, preserve=['Segment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(pruned, \"/home/koehler/Desktop/teste_pruned.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming nodes\n",
      "Transforming edges\n",
      "Removing activities\n"
     ]
    }
   ],
   "source": [
    "# Test class implementation\n",
    "my_graph = ProvenanceGraph(file_path, attributes=('dtype', 'shape', 'name', 'units', 'name', 'description', 'file_origin', 't_start', 't_stop'), annotations=('subject_name',),\n",
    "                              array_annotations={'array_annotations': ('channel_names',)}, strip_namespace=True)\n",
    "my_graph.save_gexf(\"/home/koehler/Desktop/teste_class.gexf\")\n",
    "my_graph.condense_memberships(preserve=['Segment'])\n",
    "my_graph.save_gexf(\"/home/koehler/Desktop/teste_pruned_class.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
